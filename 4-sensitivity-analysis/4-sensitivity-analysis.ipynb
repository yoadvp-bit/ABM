{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8680206c98c8ce08b341c4a8d5ece6a8",
     "grade": false,
     "grade_id": "cell-c1ea2f446e833778",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Sensitivity analysis\n",
    "\n",
    "Sensitivity analysis (SA) studies the uncertainty in the output of a model. It attempts to measure this uncertainty in the output by making small (known) changes in the inputs, in the process identifying which parameter/set of parameters has the greatest influence. This increases understanding of the relationship between input and output, which in turn can help us find errors in the model. Another purpose of SA is model simplification; model inputs that have little to no effect on the output can be fixed to specific values.\n",
    "\n",
    "In this notebook we aim to familiarise you with OFAT and Sobol SA. We will use Mesa's BatchRunner throughout the notebook and Sobol SA will be done through use of [SALib](https://github.com/SALib/SALib).\n",
    "\n",
    "First, we install the package and test if we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b305d7de6847b9d2abb6e7fe9802e834",
     "grade": false,
     "grade_id": "cell-d0df38f0204b7505",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import SALib\n",
    "clear_output()\n",
    "print(\"Everything imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import everything required for this notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from SALib.sample import saltelli\n",
    "from wolf_sheep.model import WolfSheep\n",
    "from wolf_sheep.agents import Wolf, Sheep\n",
    "from mesa.batchrunner import FixedBatchRunner\n",
    "from SALib.analyze import sobol\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "SA is only useful if we can apply it to something. Instead of having you make another model, we will use a predator-prey model that you are familiar with; the \"wolf-sheep predation model\". Specifically, [the wolf-sheep model provided in the examples provided by Mesa](https://github.com/projectmesa/mesa/tree/master/examples/wolf_sheep). You might recognise parts of it that were used as inspiration for the first notebook.\n",
    "\n",
    "From the description:\n",
    "\n",
    "''A simple ecological model, consisting of three agent types: wolves, sheep, and grass. The wolves and the sheep wander around the grid at random. Wolves and sheep both expend energy moving around, and replenish it by eating. Sheep eat grass, and wolves eat sheep if they end up on the same grid cell.\n",
    "\n",
    "If wolves and sheep have enough energy, they reproduce, creating a new wolf or sheep (in this simplified model, only one parent is needed for reproduction). The grass on each cell regrows at a constant rate. If any wolves and sheep run out of energy, they die.''\n",
    "\n",
    "The model can be found in the folder 'wolf_sheep/'. If you are interested, you can read through the code. You can even use:\n",
    "\n",
    "`mesa runserver`\n",
    "\n",
    "...to view the model in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OFAT SA\n",
    "One Factor at A Time (OFAT or OFaaT) SA is a method of testing the output variance of inputs one at a time. \n",
    "\n",
    "We will show you how to change only one variable at the time and how to plot the results. The model we use has default parameters for each of the variables, which makes this fairly easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our variables and bounds\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['sheep_reproduce', 'wolf_reproduce', 'wolf_gain_from_food'],\n",
    "    'bounds': [[0.01, 0.1], [0.01, 0.1], [5, 30]]\n",
    "}\n",
    "\n",
    "# Set the repetitions, the amount of steps, and the amount of distinct values per variable\n",
    "replicates = 30\n",
    "max_steps = 100\n",
    "distinct_samples = 30 \n",
    "\n",
    "# Set the outputs\n",
    "model_reporters = {\"Wolves\": lambda m: m.schedule.get_breed_count(Wolf),\n",
    "             \"Sheep\": lambda m: m.schedule.get_breed_count(Sheep)}\n",
    "\n",
    "data = {}\n",
    "\n",
    "for i, var in enumerate(problem['names']):\n",
    "    # Get the bounds for this variable and get <distinct_samples> samples within this space (uniform)\n",
    "    samples = np.linspace(*problem['bounds'][i], num=distinct_samples)\n",
    "    \n",
    "    # Keep in mind that wolf_gain_from_food should be integers. You will have to change\n",
    "    # your code to acommodate for this or sample in such a way that you only get integers.\n",
    "    if var == 'wolf_gain_from_food':\n",
    "        samples = np.linspace(*problem['bounds'][i], num=distinct_samples, dtype=int)\n",
    "    \n",
    "    batch = FixedBatchRunner(WolfSheep,\n",
    "                        max_steps=max_steps,\n",
    "                        iterations=replicates,\n",
    "                        parameters_list=[{var: value} for value in samples],\n",
    "                        fixed_parameters= None,\n",
    "                        model_reporters=model_reporters,\n",
    "                        display_progress=True)\n",
    "    \n",
    "    batch.run_all()\n",
    "    \n",
    "    data[var] = batch.get_model_vars_dataframe()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we can plot it! First show the influence of the variable on sheep, then on wolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_var_conf(ax, df, var, param, i):\n",
    "    \"\"\"\n",
    "    Helper function for plot_all_vars. Plots the individual parameter vs\n",
    "    variables passed.\n",
    "\n",
    "    Args:\n",
    "        ax: the axis to plot to\n",
    "        df: dataframe that holds the data to be plotted\n",
    "        var: variables to be taken from the dataframe\n",
    "        param: which output variable to plot\n",
    "    \"\"\"\n",
    "    x = df.groupby(var).mean().reset_index()[var]\n",
    "    y = df.groupby(var).mean()[param]\n",
    "\n",
    "    replicates = df.groupby(var)[param].count()\n",
    "    err = (1.96 * df.groupby(var)[param].std()) / np.sqrt(replicates)\n",
    "\n",
    "    ax.plot(x, y, c='k')\n",
    "    ax.fill_between(x, y - err, y + err)\n",
    "\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel(param)\n",
    "\n",
    "def plot_all_vars(df, param):\n",
    "    \"\"\"\n",
    "    Plots the parameters passed vs each of the output variables.\n",
    "\n",
    "    Args:\n",
    "        df: dataframe that holds all data\n",
    "        param: the parameter to be plotted\n",
    "    \"\"\"\n",
    "\n",
    "    f, axs = plt.subplots(3, figsize=(7, 10))\n",
    "    \n",
    "    for i, var in enumerate(problem['names']):\n",
    "        plot_param_var_conf(axs[i], data[var], var, param, i)\n",
    "\n",
    "for param in ('Sheep', 'Wolves'):\n",
    "    plot_all_vars(data, param)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobol SA\n",
    "\n",
    "Sobol Sensitivity Analysis ([Sobol 2001](http://www.sciencedirect.com/science/article/pii/S0378475400002706), [Saltelli 2002](http://www.sciencedirect.com/science/article/pii/S0010465502002801), [Saltelli et al. 2010](http://www.sciencedirect.com/science/article/pii/S0010465509003087)) is a global SA method that determines the contribution of each input parameter or a combination of parameters and their interaction to the overall output variance. OFAT, while it is easier to implement (and certainly is less thought-intensive), has a couple of downsides.\n",
    "\n",
    "1. OFAT requires a large amount of runs to get accurate results\n",
    "2. OFAT cannot estimate interactions of combinations of inputs\n",
    "3. OFAT can miss optimal settings of factors\n",
    "\n",
    "Sobol can find higher order interactions, but still requires a large amount of runs.\n",
    "\n",
    "### Getting the data\n",
    "\n",
    "Before we can start analysing the model, we will have to sample our data. There are multiple methods for sampling included in SALib, but since we're using Sobol, we will use Saltelli sampling for this. \n",
    "\n",
    "The following code shows how you could collect data for the \"wolf-sheep\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddf793f41d915b201811ee80a07f5185",
     "grade": false,
     "grade_id": "cell-68257ab63602b895",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from mesa.batchrunner import BatchRunner\n",
    "\n",
    "# Set the repetitions, the amount of steps, and the amount of distinct values per variable\n",
    "replicates = 10\n",
    "max_steps = 100\n",
    "distinct_samples = 10\n",
    "\n",
    "# We get all our samples here\n",
    "param_values = saltelli.sample(problem, distinct_samples)\n",
    "\n",
    "# READ NOTE BELOW CODE\n",
    "batch = BatchRunner(WolfSheep, \n",
    "                    max_steps=max_steps,\n",
    "                    variable_parameters={name:[] for name in problem['names']},\n",
    "                    model_reporters=model_reporters)\n",
    "\n",
    "count = 0\n",
    "data = pd.DataFrame(index=range(replicates*len(param_values)), \n",
    "                                columns=['sheep_reproduce', 'wolf_reproduce', 'wolf_gain_from_food'])\n",
    "data['Run'], data['Sheep'], data['Wolves'] = None, None, None\n",
    "\n",
    "for i in range(replicates):\n",
    "    for vals in param_values: \n",
    "        # Change parameters that should be integers\n",
    "        vals = list(vals)\n",
    "        vals[2] = int(vals[2])\n",
    "        # Transform to dict with parameter names and their values\n",
    "        variable_parameters = {}\n",
    "        for name, val in zip(problem['names'], vals):\n",
    "            variable_parameters[name] = val\n",
    "\n",
    "        batch.run_iteration(variable_parameters, tuple(vals), count)\n",
    "        iteration_data = batch.get_model_vars_dataframe().iloc[count]\n",
    "        iteration_data['Run'] = count # Don't know what causes this, but iteration number is not correctly filled\n",
    "        data.iloc[count, 0:3] = vals\n",
    "        data.iloc[count, 3:6] = iteration_data\n",
    "        count += 1\n",
    "\n",
    "        clear_output()\n",
    "        print(f'{count / (len(param_values) * (replicates)) * 100:.2f}% done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c10e4dc20c6b13c7f6e885e810062c4b",
     "grade": false,
     "grade_id": "cell-db4a4903984a0d77",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that even though we use the BatchRunner provided by Mesa, we do not use its full capabilities. Normally, you would set all parameters properly (at line 25) and then run the batchrunner with `batch.run_all()`. However, the batchrunner will then proceed to run every possible combination of the variables you have passed it. We already have the combinations (samples) we need, because we got those from SALib.\n",
    "\n",
    "Preferably, you would want to save the results as a csv so you can load them more easily when analysing your results. \n",
    "\n",
    "Printing the data shows (a part of) our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 800 distinct results. We have 3 variables, and we take 2 distinct values per sample per variable. A single sample from saltelli-sampling results in $2D+2$ different combinations for $D$ variables, $2(3)+2=8$ for our case (for more details, take a look at [Saltelli et al. 2010](http://www.sciencedirect.com/science/article/pii/S0010465509003087) and the [saltelli.sample() documentation](http://salib.readthedocs.io/en/latest/_modules/SALib/sample/saltelli.html)). Since we take $N=10$ samples, we get $N*(2D+2)=80$ combinations. We repeat each of these combinations 10 times, resulting in 800 results.\n",
    "Note that the above analysis includes second-order indices. For a case where you only wish to study first- and total-order effects, `calc_second_order` for `saltelli.sample()` is set to false, and a single sample is sufficiently composed of $D+2$ different combinations.\n",
    "\n",
    "We only consider 2 outputs in this example; the amount of sheep and wolves at the end of the simulation. Can you think of more outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing\n",
    "\n",
    "Now we can use the `analyze()` method provided by SALib that performs Sobol analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ccf1b8cc0ae84fbaf8e924569cc75c3",
     "grade": false,
     "grade_id": "cell-45ea4199ae821e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Si_sheep = sobol.analyze(problem, data['Sheep'].values, print_to_console=True)\n",
    "Si_wolves = sobol.analyze(problem, data['Wolves'].values, print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very insightfull. Let's make a function that can plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_index(s, params, i, title=''):\n",
    "    \"\"\"\n",
    "    Creates a plot for Sobol sensitivity analysis that shows the contributions\n",
    "    of each parameter to the global sensitivity.\n",
    "\n",
    "    Args:\n",
    "        s (dict): dictionary {'S#': dict, 'S#_conf': dict} of dicts that hold\n",
    "            the values for a set of parameters\n",
    "        params (list): the parameters taken from s\n",
    "        i (str): string that indicates what order the sensitivity is.\n",
    "        title (str): title for the plot\n",
    "    \"\"\"\n",
    "\n",
    "    if i == '2':\n",
    "        p = len(params)\n",
    "        params = list(combinations(params, 2))\n",
    "        indices = s['S' + i].reshape((p ** 2))\n",
    "        indices = indices[~np.isnan(indices)]\n",
    "        errors = s['S' + i + '_conf'].reshape((p ** 2))\n",
    "        errors = errors[~np.isnan(errors)]\n",
    "    else:\n",
    "        indices = s['S' + i]\n",
    "        errors = s['S' + i + '_conf']\n",
    "        plt.figure()\n",
    "\n",
    "    l = len(indices)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylim([-0.2, len(indices) - 1 + 0.2])\n",
    "    plt.yticks(range(l), params)\n",
    "    plt.errorbar(indices, range(l), xerr=errors, linestyle='None', marker='o')\n",
    "    plt.axvline(0, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first plot 1st, 2nd, and total-order sensitivity for the output variable 'Sheep', then for 'Wolves'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Si in (Si_sheep, Si_wolves):\n",
    "    # First order\n",
    "    plot_index(Si, problem['names'], '1', 'First order sensitivity')\n",
    "    plt.show()\n",
    "\n",
    "    # Second order\n",
    "    plot_index(Si, problem['names'], '2', 'Second order sensitivity')\n",
    "    plt.show()\n",
    "\n",
    "    # Total order\n",
    "    plot_index(Si, problem['names'], 'T', 'Total order sensitivity')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
